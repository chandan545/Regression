{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions"
      ],
      "metadata": {
        "id": "5_ethaT1csRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "   - Simple Linear Regression is a way to find a relationship between two variables:\n",
        "      - One independent variable (X) → The input or cause\n",
        "      - One dependent variable (Y) → The output or effect\n",
        "   - It helps us predict Y based on X using a straight-line equation:\n",
        "\n",
        "        𝑌=𝑚𝑋+𝑐\n",
        "   - Where:\n",
        "      - m = Slope of the line (shows how much Y changes for a unit change in X)\n",
        "      - c = Intercept (value of Y when X = 0)\n",
        "   - Example:\n",
        "      - Imagine you want to predict a student's exam score (Y) based on study hours (X). If we analyze past data and find the equation:\n",
        "      - This means:\n",
        "        - Each extra hour of study increases the score by 10\n",
        "        - A student who doesn’t study (0 hours) gets a score of 20\n",
        "      - Simple Linear Regression helps in predictions and understanding relationships between two variables!\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "   - Simple Linear Regression relies on four key assumptions:\n",
        "     - Linearity → The relationship between X (independent variable) and Y (dependent variable) is a straight line.\n",
        "     - Independence → The observations are independent of each other (no pattern in errors).\n",
        "     - Homoscedasticity → The variance of errors (difference between actual and predicted Y) is constant across all values of X.\n",
        "     - Normality of Errors → The errors (residuals) follow a normal distribution.\n",
        "   - These assumptions help ensure accurate predictions and valid conclusions!\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "  - The coefficient 𝑚 in the equation Y=mX+c represents the slope of the regression line.\n",
        "  - Meaning of m:\n",
        "    - It shows how much Y changes for a one-unit increase in X.\n",
        "    - If m is positive, Y increases as X increases.\n",
        "    - If m is negative, Y decreases as X increases.\n",
        "  - Example:\n",
        "    - If the equation is:\n",
        "                  Salary=5000×Years of Experience+30000\n",
        "    - Here, m=5000, which means:\n",
        "      - Each extra year of experience increases salary by 5000.\n",
        "    - So,m represents the rate of change of Y with respect to X!\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "   - The intercept 𝑐 in the equation Y=mX+c represents the value of Y when X = 0.\n",
        "   - Meaning of c:\n",
        "     - It is the point where the regression line crosses the Y-axis.\n",
        "     - It shows the starting value of Y when there is no influence from X.\n",
        "   - Example:\n",
        "     - If the equation is:\n",
        "                   Salary=5000×Years of Experience+30000\n",
        "   - Here, c=30000, which means:\n",
        "     - A person with 0 years of experience is predicted to have a salary of 30,000.\n",
        "   - So, c is the baseline value of Y before X has any effect!\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "   - The slope m in Simple Linear Regression is calculated using the formula:\n",
        "   - $m = \\frac{n \\sum (X Y) - \\sum X \\sum Y}{n \\sum X^2 - (\\sum X)^2}$\n",
        "   - Steps to Calculate m:\n",
        "     - Find the mean (average) of X and Y.\n",
        "     - Use the formula to compute the slope based on the data points.\n",
        "   - Intuition:\n",
        "     - Numerator: Measures how X and Y vary together.\n",
        "     - Denominator: Measures how X varies by itself.\n",
        "   - m tells us how much Y changes for each unit increase in X!\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "   - The purpose of the Least Squares Method in Simple Linear Regression is to find the best-fitting line by minimizing the difference between actual and predicted values.\n",
        "   - How It Works:\n",
        "     - It calculates the sum of squared errors (residuals):\n",
        "        $\\sum (Y_{\\text{actual}} - Y_{\\text{predicted}})^2$\n",
        "     - The goal is to find m (slope) and c (intercept) that make this sum as small as possible.\n",
        "   - Why Use It?\n",
        "     - Ensures the best possible predictions by reducing errors.\n",
        "     - Finds the most accurate linear relationship between X and Y.\n",
        "   - In short: The Least Squares Method helps create the most accurate regression line by minimizing prediction errors!\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "   - The coefficient of determination (R²) measures how well the regression line fits the data.\n",
        "     - R² = 1 → Perfect fit (100% of the variance in Y is explained by X).\n",
        "     - R² = 0 → No relationship (X does not explain Y at all).\n",
        "     - Higher R² → Better model fit.\n",
        "   - It tells us the percentage of variation in Y explained by X in Simple Linear Regression.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "   - Multiple Linear Regression is an extension of Simple Linear Regression where we use multiple independent variables (X₁, X₂, X₃, ...) to predict a dependent variable (Y).\n",
        "   - Equation:\n",
        "        $\\sum (Y_{\\text{actual}} - Y_{\\text{predicted}})^2$\n",
        "   - Example:\n",
        "     - Predicting house price (Y) based on:\n",
        "       - Size (X₁)\n",
        "       - Location (X₂)\n",
        "       - Number of rooms (X₃)\n",
        "   - In short: It helps analyze how multiple factors influence a single outcome!\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "   - The main difference between Simple and Multiple Linear Regression is:\n",
        "    - Simple Linear Regression has only one independent variable to predict the outcome.\n",
        "    - Multiple Linear Regression has two or more independent variables to predict the outcome.\n",
        "  - In short: Simple uses one factor, while Multiple considers multiple factors to make predictions.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "   - The key assumptions of Multiple Linear Regression are:\n",
        "     - Linearity → The relationship between independent variables and the dependent variable is linear.\n",
        "     - Independence → The observations are independent of each other.\n",
        "     - No Multicollinearity → Independent variables should not be highly correlated with each other.\n",
        "     - Homoscedasticity → The variance of errors (residuals) should be constant across all values of independent variables.\n",
        "     - Normality of Errors → The residuals (differences between actual and predicted values) should follow a normal distribution.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "   - Heteroscedasticity:\n",
        "     - Heteroscedasticity occurs when the variance of errors (residuals) is not constant across all levels of the independent variables. This means the spread of errors increases or decreases as the values of independent variables change.\n",
        "   - Effects on Multiple Linear Regression:\n",
        "     - Biased Standard Errors → Can lead to incorrect p-values, affecting hypothesis testing.\n",
        "     - Unreliable Coefficients → Reduces the accuracy of confidence intervals.\n",
        "     - Poor Model Predictions → The regression model may not generalize well to new data.\n",
        "   - Solution:\n",
        "    - Use log transformation of the dependent variable.\n",
        "    - Apply robust standard errors or Weighted Least Squares (WLS) regression.\n",
        "   - In short: Heteroscedasticity makes the model less reliable by distorting error estimates!\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "    - Ways to Improve a Multiple Linear Regression Model with High Multicollinearity\n",
        "      - Remove Highly Correlated Variables → Identify and drop one of the highly correlated independent variables.\n",
        "      - Use Variance Inflation Factor (VIF) → Remove variables with high VIF values (VIF > 10 indicates strong multicollinearity).\n",
        "      - Feature Engineering → Combine correlated variables into a single meaningful variable (e.g., averaging or principal component analysis).\n",
        "      - Regularization Techniques → Apply Ridge Regression (L2 penalty) or Lasso Regression (L1 penalty) to reduce the impact of multicollinearity.\n",
        "      - Increase Sample Size → Collect more data to reduce the effects of collinearity.\n",
        "   - In short: Identify and reduce redundant variables, apply regularization, or use alternative regression methods!\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "    - Common Techniques for Transforming Categorical Variables in Regression Models\n",
        "      - One-Hot Encoding → Converts each category into a separate binary (0/1) column. Best for nominal (unordered) categories.\n",
        "      - Label Encoding → Assigns a unique number to each category (e.g., \"Red\" = 1, \"Blue\" = 2). Works well for ordinal (ordered) categories.\n",
        "      - Ordinal Encoding → Similar to label encoding but maintains a meaningful order (e.g., \"Low\" = 1, \"Medium\" = 2, \"High\" = 3).\n",
        "      - Binary Encoding → Converts categories into binary values and stores them in fewer columns, reducing dimensionality.\n",
        "      - Frequency Encoding → Replaces categories with their occurrence count in the dataset.\n",
        "      - Target Encoding (Mean Encoding) → Replaces categories with the mean of the target variable (used cautiously to avoid data leakage).\n",
        "   - In short: Use One-Hot for nominal data and Ordinal/Label for ordered categories. Other methods help when dealing with high-cardinality data!\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "    - Interaction terms capture the effect of two or more independent variables working together to influence the dependent variable. They help in understanding combined effects beyond individual contributions.\n",
        "    - Why Use Interaction Terms?\n",
        "      - Detect Variable Relationships → Some variables may only impact the outcome when combined.\n",
        "      - Improve Model Accuracy → Adds complexity to better explain patterns in data.\n",
        "      - Uncover Hidden Insights → Helps reveal dependencies that a standard model might miss.\n",
        "    - Example:\n",
        "      - Without Interaction: Salary depends on Experience and Education separately.\n",
        "      - With Interaction: The effect of Experience on Salary changes based on Education level.\n",
        "    - In short: Interaction terms help model combined effects of variables, improving predictions and insights!\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "    - Interpretation of the Intercept in Simple vs. Multiple Linear Regression\n",
        "      - In Simple Linear Regression:\n",
        "        - The intercept represents the predicted value of Y when X = 0.\n",
        "        - Example: If predicting salary based on experience, the intercept is the salary of a person with 0 years of experience.\n",
        "    - In Multiple Linear Regression:\n",
        "      - The intercept represents the predicted value of Y when all independent variables (X₁, X₂, ...) are 0.\n",
        "      - Its meaning depends on whether X = 0 makes sense in the real-world context. Sometimes, it may not be interpretable.\n",
        "    - In short: In Simple Regression, the intercept is the baseline value of Y when X = 0, while in Multiple Regression, it represents Y when all predictors are 0 (which may not always be meaningful).\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "   - Significance of the Slope in Regression Analysis\n",
        "     - The slope in regression analysis shows the rate of change of the dependent variable (Y) for a one-unit increase in the independent variable (X), assuming all other factors remain constant.\n",
        "   - How It Affects Predictions:\n",
        "     - Positive Slope → As X increases, Y increases (direct relationship).\n",
        "     - Negative Slope → As X increases, Y decreases (inverse relationship).\n",
        "     - Zero Slope → No relationship between X and Y (X has no effect on Y).\n",
        "   - Example:\n",
        "     - If the slope in a salary prediction model is 5000, it means each extra year of experience increases salary by $5000.\n",
        "   - In short: The slope shows how much Y changes per unit change in X, directly impacting predictions!\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "    - Role of the Intercept in a Regression Model\n",
        "      - The intercept represents the predicted value of the dependent variable (Y) when all independent variables (X) are zero. It provides context for understanding the baseline level of Y before any effects from X are applied.\n",
        "    - How It Provides Context:\n",
        "      - Baseline Value → Helps interpret where the regression line starts.\n",
        "      - Real-World Meaning → Sometimes meaningful (e.g., base salary without experience), sometimes unrealistic (e.g., height prediction at age 0).\n",
        "      - Comparison Tool → Allows comparison of different models by showing their starting points.\n",
        "    - Example:\n",
        "      - In a salary prediction model, the intercept might represent the starting salary when experience = 0.\n",
        "    - In short: The intercept gives the starting value of Y, helping set the context for variable relationships!\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "    - Limitations of Using R² as the Sole Measure of Model Performance\n",
        "      - Does Not Indicate Causation → A high R² does not mean X causes Y; correlation ≠ causation.\n",
        "      - Ignores Model Complexity → Adding more variables can artificially increase R², even if they don't improve predictions.\n",
        "      - Not Useful for Non-Linear Relationships → R² may be low even if the model fits well in non-linear cases.\n",
        "      - Sensitive to Outliers → A few extreme values can distort R², making the model seem better or worse than it is.\n",
        "      - Does Not Assess Predictive Power → A high R² does not guarantee good predictions on new data (overfitting risk).\n",
        "   - In short: R² is helpful but should be used with other metrics like Adjusted R², RMSE, or Cross-Validation for a complete performance check!\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "    - Interpretation of a Large Standard Error for a Regression Coefficient\n",
        "      - A large standard error means that the estimated regression coefficient is highly variable and less reliable. This suggests:\n",
        "        - Low Precision → The coefficient estimate is uncertain and could vary significantly with different samples.\n",
        "        - Weak Relationship → The predictor variable may not strongly influence the dependent variable.\n",
        "        - Multicollinearity Issue → If independent variables are highly correlated, standard errors can inflate.\n",
        "        - Small Sample Size → Fewer data points can lead to higher standard errors.\n",
        "    - Effect on Interpretation:\n",
        "        - A large standard error means the coefficient is not statistically significant, making it harder to draw conclusions.\n",
        "        - It results in wide confidence intervals, indicating uncertainty in predictions.\n",
        "    -  In short: A large standard error means the coefficient is unstable and less reliable, requiring further investigation!\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "   - Identifying Heteroscedasticity in Residual Plots\n",
        "     - Heteroscedasticity can be detected using residual plots, where you plot residuals (errors) vs. predicted values.\n",
        "   -  Signs of Heteroscedasticity:\n",
        "     - Funnel Shape → Residuals spread out as predicted values increase.\n",
        "     - Uneven Variance → Residuals show increasing or decreasing patterns instead of being randomly scattered.\n",
        "     - Patterned Residuals → Clear curves or clusters instead of a uniform spread.\n",
        "   - Why Is It Important to Address?\n",
        "     - Biased Standard Errors → Can lead to incorrect statistical inferences.\n",
        "     - Unreliable Confidence Intervals → Reduces trust in coefficient estimates.\n",
        "     - Poor Prediction Accuracy → Affects the generalization of the model to new data.\n",
        "   - Solutions:\n",
        "     - Apply log transformation or weighted least squares (WLS).\n",
        "     - Use robust standard errors to adjust for heteroscedasticity.\n",
        "   - In short: Heteroscedasticity makes the model unreliable, and fixing it ensures accurate predictions and valid statistical conclusions!\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "    - High R² but Low Adjusted R² in Multiple Linear Regression\n",
        "      - If a model has a high R² but a low Adjusted R², it suggests that some independent variables are not truly contributing to explaining the dependent variable.\n",
        "    - What This Means:\n",
        "      - Overfitting → The model may include irrelevant variables that increase R² artificially.\n",
        "      - Useless Predictors → Some independent variables do not improve the model and just add noise.\n",
        "      - Multicollinearity → Highly correlated predictors can inflate R² without improving predictive power.\n",
        "      - Small Sample Size → A few data points with too many predictors can make Adjusted R² drop.\n",
        "   - Why It Matters:\n",
        "      - R² always increases when adding more variables, even if they are irrelevant.\n",
        "      - Adjusted R² penalizes unnecessary predictors, ensuring only meaningful variables improve the model.\n",
        "   - Solution:\n",
        "      - Use feature selection (e.g., stepwise regression, VIF analysis).\n",
        "      - Remove insignificant variables to improve model quality.\n",
        "   -  In short: High R² but low Adjusted R² means too many unnecessary predictors, leading to overfitting and misleading model performance!\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "    - Importance of Scaling Variables in Multiple Linear Regression\n",
        "      - Scaling variables ensures that all features contribute equally to the model and prevents numerical issues.\n",
        "    - Why Scaling is Important:\n",
        "      - Improves Interpretation → Standardized coefficients allow easy comparison of variable importance.\n",
        "      - Prevents Dominance of Large-Scale Features → Features with larger ranges (e.g., income in dollars vs. age in years) may dominate the model.\n",
        "      - Speeds Up Convergence → Some optimization algorithms (like Gradient Descent) work faster with scaled data.\n",
        "      - Reduces Multicollinearity → Standardizing variables can help in detecting and managing collinearity.\n",
        "      - Better Handling of Regularization → Ridge and Lasso regression perform better with scaled features.\n",
        "   - Common Scaling Methods:\n",
        "      - Standardization (Z-score scaling): Converts data to have mean = 0 and standard deviation = 1.\n",
        "      - Min-Max Scaling: Rescales values between 0 and 1.\n",
        "   -  In short: Scaling ensures fair contribution of variables, better performance, and improved model stability!\n",
        "\n",
        "23. What is polynomial regression?\n",
        "    - Polynomial Regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an n-degree polynomial. It is used when the relationship is non-linear but can be approximated using polynomial terms.\n",
        "    - Key Features:\n",
        "      - Captures Curved Relationships → Unlike linear regression, it can fit curves.\n",
        "      - Higher-Degree Terms → Adds squared (X²), cubic (X³), or higher-order terms to the model.\n",
        "      - Still Considered Linear Regression → Despite the polynomial terms, it remains a linear model in terms of coefficients.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "    - Difference Between Polynomial and Linear Regression\n",
        "      - Linear Regression → Models a straight-line relationship between X and Y.\n",
        "      - Polynomial Regression → Models a curved relationship by adding squared, cubic, or higher-degree terms of X.\n",
        "    - Key Difference: Linear regression fits a straight line, while polynomial regression captures non-linear trends!\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "    - Use Polynomial Regression When:\n",
        "      - The relationship between X and Y is non-linear (not a straight line).\n",
        "      - A linear model does not fit the data well (residuals show a pattern).\n",
        "      - You need to capture curves or bends in the data.\n",
        "      - The data follows a parabolic, cubic, or higher-order trend.\n",
        "    - In short: Use it when a straight line isn’t enough to explain the relationship!\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "   - The general equation for a polynomial regression model of degree n is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑎\n",
        "0\n",
        "+\n",
        "𝑎\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑎\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑎\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑎\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=a\n",
        "0\n",
        "​\n",
        " +a\n",
        "1\n",
        "​\n",
        " X+a\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +a\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+a\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ\n",
        "\n",
        "Where:\n",
        "- Y = Dependent variable (target)\n",
        "\n",
        "- X = Independent variable (feature)\n",
        "\n",
        "- a₀, a₁, a₂, …, aₙ = Coefficients\n",
        "\n",
        "- X², X³, …, Xⁿ = Polynomial terms\n",
        "\n",
        "- ϵ = Error term (captures randomness in the data)\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "    - Yes! Polynomial regression can be extended to multiple variables, creating a Multivariable Polynomial Regression model.\n",
        "    - How It Works:\n",
        "      - Instead of just one independent variable (X), you have multiple variables (X₁, X₂, X₃, ...).\n",
        "      - Polynomial terms (squared, cubic, etc.) are applied to each variable individually or in combination.\n",
        "    - When to Use It?\n",
        "      - When the relationship between the dependent variable and multiple independent variables is non-linear.\n",
        "      -  In short: Polynomial regression works for multiple variables by adding polynomial terms for each variable and their interactions!\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "    - Limitations of Polynomial Regression\n",
        "      - Overfitting → High-degree polynomials can fit the training data too well but fail on new data.\n",
        "      - Computational Complexity → Higher-degree models require more calculations and can be slow.\n",
        "      - Extrapolation Issues → Predictions outside the training range can be highly unreliable.\n",
        "      - Multicollinearity → Adding polynomial terms increases correlation between variables, affecting stability.\n",
        "      - Difficult Interpretation → Higher-degree equations become complex and harder to understand.\n",
        "    -  In short: Polynomial regression is useful for non-linear patterns, but too many terms can lead to overfitting and complexity!\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "   - When selecting the degree of a polynomial, you can use these methods to ensure the best balance between accuracy and generalization:\n",
        "     - R² and Adjusted R² → Measures how well the model explains variance, but higher degrees may inflate R² without improving prediction.\n",
        "     - Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) → Lower values indicate a better fit with less error.\n",
        "     - Cross-Validation (e.g., k-Fold CV) → Tests model performance on different data splits to prevent overfitting.\n",
        "     - Akaike Information Criterion (AIC) & Bayesian Information Criterion (BIC) → Penalizes excessive complexity to avoid overfitting.\n",
        "     - Residual Plots → Checks for patterns in errors; a well-fitted model should have randomly scattered residuals.\n",
        "     - Training vs. Test Error → A big gap suggests overfitting; choose the degree where both errors are minimized.\n",
        "  - In short: Use MSE, R², cross-validation, and residual analysis to find the best polynomial degree without overfitting!\n",
        "\n",
        "30. why is visualization important in polynomial regression?\n",
        "    - Visualization helps in understanding the model's performance and selecting the right polynomial degree.\n",
        "    - Why It’s Important:\n",
        "      - Detects Underfitting or Overfitting → A plot of the polynomial curve against the data helps identify if the model is too simple or too complex.\n",
        "      - Residual Analysis → Residual plots show whether errors are randomly distributed or if patterns exist (indicating a poor fit).\n",
        "      - Comparing Different Degrees → Helps visually assess which polynomial degree best fits the data without unnecessary complexity.\n",
        "      - Identifies Outliers → Scatter plots can highlight extreme data points affecting the regression.\n",
        "      - Improves Interpretability → Seeing the curve makes it easier to understand how the model captures trends.\n",
        "   - In short: Visualization makes it easier to evaluate model fit, detect issues, and improve interpretability in polynomial regression!\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "    - Polynomial regression can be implemented using scikit-learn in a few simple steps.\n",
        "    - Steps to Implement Polynomial Regression\n",
        "      - Import Required Libraries\n",
        "      - Generate or Load Data\n",
        "      - Transform Data into Polynomial Features\n",
        "      - Fit a Linear Regression Model\n",
        "      - Make Predictions and Visualize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BYu3j_weMCkh"
      }
    }
  ]
}